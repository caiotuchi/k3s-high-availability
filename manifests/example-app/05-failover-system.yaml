# ==========================================================
# SISTEMA DE FAILOVER AUTOM√ÅTICO (Somente C2)
# ==========================================================

# 1. Service Entrypoint (O Backend conecta AQUI)
apiVersion: v1
kind: Service
metadata:
  name: backend-db-entrypoint
  namespace: default
  annotations:
    # CORRE√á√ÉO CR√çTICA: Permite buscar endpoints no Cluster 01
    service.cilium.io/global: "true"
spec:
  ports:
  - port: 5432
    targetPort: 5432
    name: postgres
  selector:
    # Estado inicial
    app: postgres
    role: primary

---
# 2. ServiceAccount para o Rob√¥
apiVersion: v1
kind: ServiceAccount
metadata:
  name: failover-bot
  namespace: default

---
# 3. Permiss√£o para Alterar Servi√ßos
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: service-patcher
  namespace: default
rules:
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list", "patch", "update"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: failover-bot-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: failover-bot
  namespace: default
roleRef:
  kind: Role
  name: service-patcher
  apiGroup: rbac.authorization.k8s.io

---
# 4. O Rob√¥ de Monitoramento (Watcher v2 - Com Anti-Flapping)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: failover-watcher
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: failover-watcher
  template:
    metadata:
      labels:
        app: failover-watcher
    spec:
      serviceAccountName: failover-bot
      containers:
      - name: watcher
        image: bitnami/kubectl:latest
        command:
        - /bin/bash
        - -c
        - |
          echo "ü§ñ Iniciando Monitor de Failover (v2 - Robusto)..."
          
          MASTER_HOST="postgres-global-master.default.svc.cluster.local"
          SERVICE_NAME="backend-db-entrypoint"
          NAMESPACE="default"
          
          # Vari√°veis de Controle
          FAIL_COUNT=0
          MAX_RETRIES=3  # Precisa falhar 3x (aprox 15s) para ativar failover
          
          while true; do
            # Teste de conex√£o TCP (timeout curto)
            if timeout 3s bash -c "</dev/tcp/$MASTER_HOST/5432" 2>/dev/null; then
              STATUS="ONLINE"
              FAIL_COUNT=0 # Reseta contagem se tiver sucesso
            else
              STATUS="OFFLINE"
              FAIL_COUNT=$((FAIL_COUNT+1))
              echo "‚ö†Ô∏è  Alerta: Falha na conex√£o com Master ($FAIL_COUNT/$MAX_RETRIES)"
            fi
            
            # Pega configura√ß√£o atual do servi√ßo
            CURRENT_ROLE=$(kubectl get svc $SERVICE_NAME -n $NAMESPACE -o jsonpath='{.spec.selector.role}')
            
            # L√ìGICA DE DECIS√ÉO
            if [ "$STATUS" == "ONLINE" ]; then
                # Se Master est√° online e o servi√ßo est√° apontando para r√©plica (Modo Failover)
                if [ "$CURRENT_ROLE" != "primary" ]; then
                    echo "üü¢ Master Est√°vel! Revertendo para Escrita (Primary)..."
                    kubectl patch svc $SERVICE_NAME -n $NAMESPACE -p '{"spec":{"selector":{"role":"primary"}}}'
                fi
            
            else
                # Se Master est√° OFFLINE
                if [ $FAIL_COUNT -ge $MAX_RETRIES ]; then
                    # S√≥ muda se j√° n√£o estiver em modo r√©plica
                    if [ "$CURRENT_ROLE" != "replica-remote" ]; then
                        echo "üî¥ Master CONFIRMADO OFFLINE ($FAIL_COUNT falhas)! Ativando Failover..."
                        kubectl patch svc $SERVICE_NAME -n $NAMESPACE -p '{"spec":{"selector":{"role":"replica-remote"}}}'
                    fi
                fi
            fi
            
            sleep 5
          done